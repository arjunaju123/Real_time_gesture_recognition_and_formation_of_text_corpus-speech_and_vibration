# Real_time_gesture_recognition_and_formation_of_text_corpus-speech_and_vibration

# Sign Language Recognition Using Deep Learning

Sign language plays a vital role in the differently-abled community. Although it was developed in the early 18th century, the widespread use of this language still needs improvement due to the difficulty in interpreting it for ordinary people. This project addresses this issue by leveraging deep computer vision techniques, specifically Convolutional Neural Networks (CNN).

## Introduction

The primary objective of this project is to enable real-time recognition of the American Sign Language (ASL) and custom-made gestures using the latest Mediapipe technology. We have developed two transfer learning models for this purpose:

1. **ASL Character Recognition**: This model is trained to recognize 26 ASL characters and an additional SPACE character, totaling 27 gestures.

2. **Custom Gesture Recognition**: In addition to ASL characters, we have also trained a separate model to recognize six custom-made gestures.

By combining these two models, we can achieve real-time hand gesture recognition, which can be further used to build a text corpus and construct sentences.

## Methodology

### Data Collection and Preprocessing

We collected a diverse dataset of ASL and custom-made gestures to train the models. The data was carefully preprocessed to ensure optimal training results.

### Convolutional Neural Networks (CNN)

We used Convolutional Neural Networks due to their effectiveness in image recognition tasks. Transfer learning was employed to leverage pre-trained models for efficient training.

### Real-Time Recognition

With the use of Mediapipe technology, we achieved real-time recognition of hand gestures, allowing for seamless interaction.

## Features and Functionalities

1. **Text Corpus Generation**: The recognized words and characters can be converted into a text corpus, enabling communication in sign language.

2. **Voice Conversion**: The model includes functionality to convert recognized words and characters into voice, aiding both the differently-abled and ordinary users.

3. **Vibration Patterns**: For individuals who are both deaf and blind, the system is equipped to convert gestures into patterns of vibration, facilitating communication.

## Model Performance

The proposed transfer learning model has demonstrated impressive accuracy, achieving 99% accuracy for both the ASL character recognition and custom gesture recognition models.

![Sample Image](project_poster_group8_page-0001.jpg)

*Note: Add a relevant image here to showcase the sign language recognition system in action.*

## Conclusion

This project demonstrates the potential of deep computer vision techniques in enhancing the accessibility and widespread use of sign language. The combination of ASL character recognition and custom gesture recognition models provides a versatile and efficient system for real-time sign language interpretation and communication.

If you are interested in contributing to this project or have any feedback, please feel free to reach out!

*Author: [ARJUN S](https://github.com/arjunaju123)*
